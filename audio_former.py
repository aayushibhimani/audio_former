# -*- coding: utf-8 -*-
"""m23cse001.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oKWi4maW9laHQoe8geEIWh9sFFwrEPtL
"""

# # Installing the requirements
# print('Installing Requirements... ',end='')
# !pip install lightning
# print('Done')

# Installing the requirements
print('Installing Requirements... ',end='')
!pip install lightning --quiet
!pip install torchaudio --quiet
!pip install torchsummary --quiet
!pip install torchviz --quiet
!pip install wandb --quiet
!pip install seaborn --quiet
print('Done')

# Importing Libraries
print('Importing Libraries... ',end='')
import os
from pathlib import Path
import pandas as pd
import torchaudio
import zipfile
from torchaudio.transforms import Resample
import IPython.display as ipd
from matplotlib import pyplot as plt
from tqdm import tqdm
import pytorch_lightning as pl
from torch.utils.data import Dataset, DataLoader
import torch
from IPython.display import Audio
import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from tqdm import tqdm
from torch.utils.data import TensorDataset
from torch.utils.data import DataLoader
from torch.utils.data import random_split
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR
import torch.nn.functional as F
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
#import seaborn as sns
import matplotlib.pyplot as plt
from torchvision.utils import make_grid
print('Done')

from google.colab import drive
drive.mount('/content/drive')

# with zipfile.ZipFile("dataset.zip", 'r') as zip_ref:
#     extracted_files = zip_ref.extractall("data_extracted")
# print(extracted_files)

# Loading dataset
path = Path('/content/drive/MyDrive/data_dl2/data_extracted')
df = pd.read_csv('/content/drive/MyDrive/data_dl2/data_extracted/meta/esc50.csv')

# Getting list of raw audio files
wavs = list(path.glob('audio/*'))  # List all audio files in the 'audio' directory using pathlib.Path.glob

# Visualizing data
waveform, sample_rate = torchaudio.load(wavs[0])  # Load the waveform and sample rate of the first audio file using torchaudio

print("Shape of waveform: {}".format(waveform.size()))  # Print the shape of the waveform tensor
print("Sample rate of waveform: {}".format(sample_rate))  # Print the sample rate of the audio file

# Plot the waveform using matplotlib
plt.figure()
plt.plot(waveform.t().numpy())  # Transpose and convert the waveform tensor to a NumPy array for plotting

# Display the audio using IPython.display.Audio
ipd.Audio(waveform, rate=sample_rate)  # Create an interactive audio player for the loaded waveform

class CustomDataset(Dataset):
    def __init__(self, dataset, **kwargs):
        # Initialize CustomDataset object with relevant parameters
        # dataset: "train", "val", or "test"
        # kwargs: Additional parameters like data directory, dataframe, folds, etc.

        # Extract parameters from kwargs
        self.data_directory = kwargs["data_directory"]
        self.data_frame = kwargs["data_frame"]
        self.validation_fold = kwargs["validation_fold"]
        self.testing_fold = kwargs["testing_fold"]
        self.esc_10_flag = kwargs["esc_10_flag"]
        self.file_column = kwargs["file_column"]
        self.label_column = kwargs["label_column"]
        self.sampling_rate = kwargs["sampling_rate"]
        self.new_sampling_rate = kwargs["new_sampling_rate"]
        self.sample_length_seconds = kwargs["sample_length_seconds"]

        # Filter dataframe based on esc_10_flag and data_type
        if self.esc_10_flag:
            self.data_frame = self.data_frame.loc[self.data_frame['esc10'] == True]

        if dataset == "train":
            self.data_frame = self.data_frame.loc[
                (self.data_frame['fold'] != self.validation_fold) & (self.data_frame['fold'] != self.testing_fold)]
        elif dataset == "val":
            self.data_frame = self.data_frame.loc[self.data_frame['fold'] == self.validation_fold]
        elif dataset == "test":
            self.data_frame = self.data_frame.loc[self.data_frame['fold'] == self.testing_fold]

        # Get unique categories from the filtered dataframe
        self.categories = sorted(self.data_frame[self.label_column].unique())

        # Initialize lists to hold file names, labels, and folder numbers
        self.file_names = []
        self.labels = []

        # Initialize dictionaries for category-to-index and index-to-category mapping
        self.category_to_index = {}
        self.index_to_category = {}

        for i, category in enumerate(self.categories):
            self.category_to_index[category] = i
            self.index_to_category[i] = category

        # Populate file names and labels lists by iterating through the dataframe
        for ind in tqdm(range(len(self.data_frame))):
            row = self.data_frame.iloc[ind]
            file_path = self.data_directory / "audio" / row[self.file_column]
            self.file_names.append(file_path)
            self.labels.append(self.category_to_index[row[self.label_column]])

        self.resampler = torchaudio.transforms.Resample(self.sampling_rate, self.new_sampling_rate)

        # Window size for rolling window sample splits (unfold method)
        if self.sample_length_seconds == 2:
            self.window_size = self.new_sampling_rate * 2
            self.step_size = int(self.new_sampling_rate * 0.75)
        else:
            self.window_size = self.new_sampling_rate
            self.step_size = int(self.new_sampling_rate * 0.5)

    def __getitem__(self, index):
        # Split audio files with overlap, pass as stacked tensors tensor with a single label
        path = self.file_names[index]
        audio_file = torchaudio.load(path, format=None, normalize=True)
        audio_tensor = self.resampler(audio_file[0])
        splits = audio_tensor.unfold(1, self.window_size, self.step_size)
        samples = splits.permute(1, 0, 2)
        return samples, self.labels[index]

    def __len__(self):
        return len(self.file_names)

class CustomDataModule(pl.LightningDataModule):
    def __init__(self, **kwargs):
        # Initialize the CustomDataModule with batch size, number of workers, and other parameters
        super().__init__()
        self.batch_size = kwargs["batch_size"]
        self.num_workers = kwargs["num_workers"]
        self.data_module_kwargs = kwargs

    def setup(self, stage=None):
        # Define datasets for training, validation, and testing during Lightning setup

        # If in 'fit' or None stage, create training and validation datasets
        if stage == 'fit' or stage is None:
            self.training_dataset = CustomDataset(dataset="train", **self.data_module_kwargs)
            self.validation_dataset = CustomDataset(dataset="val", **self.data_module_kwargs)

        # If in 'test' or None stage, create testing dataset
        if stage == 'test' or stage is None:
            self.testing_dataset = CustomDataset(dataset="test", **self.data_module_kwargs)

    def train_dataloader(self):
        # Return DataLoader for training dataset
        return DataLoader(self.training_dataset,
                          batch_size=self.batch_size,
                          shuffle=True,
                          collate_fn=self.collate_function,
                          num_workers=self.num_workers)

    def val_dataloader(self):
        # Return DataLoader for validation dataset
        return DataLoader(self.validation_dataset,
                          batch_size=self.batch_size,
                          shuffle=False,
                          collate_fn=self.collate_function,
                          num_workers=self.num_workers)

    def test_dataloader(self):
        # Return DataLoader for testing dataset
        return DataLoader(self.testing_dataset,
                          batch_size=32,
                          shuffle=False,
                          collate_fn=self.collate_function,
                          num_workers=self.num_workers)

    def collate_function(self, data):
        """
        Collate function to process a batch of examples and labels.

        Args:
            data: a tuple of 2 tuples with (example, label) where
                example are the split 1 second sub-frame audio tensors per file
                label = the label

        Returns:
            A list containing examples (concatenated tensors) and labels (flattened tensor).
        """
        examples, labels = zip(*data)
        examples = torch.stack(examples).squeeze(2)
        labels = torch.flatten(torch.tensor(labels))

        return [examples, labels]

# Data Setup
test_samp = 1
valid_samp = 2 # Use any value ranging from 2 to 5 for k-fold validation (valid_fold)
batch_size = 32 # Free to change
num_workers = 2 # Free to change
custom_data_module = CustomDataModule(batch_size=batch_size,
                                      num_workers=num_workers,
                                      data_directory=path,
                                      data_frame=df,
                                      validation_fold=valid_samp,
                                      testing_fold=test_samp,  # set to 0 for no test set
                                      esc_10_flag=True,
                                      file_column='filename',
                                      label_column='category',
                                      sampling_rate=44100,
                                      new_sampling_rate=16000,  # new sample rate for input
                                      sample_length_seconds=1  # new length of input in seconds
                                      )

custom_data_module.setup()

# Data Exploration
print('Class Label: ', custom_data_module.training_dataset[0][1])  # this prints the class label
print('Shape of data sample tensor: ', custom_data_module.training_dataset[0][0].shape)  # this prints the shape of the sample (Frames, Channel, Features)

# Dataloader(s)
x = next(iter(custom_data_module.train_dataloader()))
y = next(iter(custom_data_module.val_dataloader()))
z = next(iter(custom_data_module.test_dataloader()))
print('Train Dataloader:')
print('Input Samples Size:', x[0].size())  # Print the size of the input samples tensor
print('Labels Size:', x[1].size())
print('Train Dataloader:')
print(x)
print('Validation Dataloader:')
print(y)
print('Test Dataloader:')
print(z)

"""Architecture 1"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class CNNModel(nn.Module):
    def __init__(self):
        super(CNNModel, self).__init__()
        self.layer1 = nn.Sequential(
            nn.Conv1d(in_channels=9, out_channels=16, kernel_size=7, stride=1, padding=3),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2),
            nn.Dropout(0.3)
        )
        self.layer2 = nn.Sequential(
            nn.Conv1d(in_channels=16, out_channels=8, kernel_size=5, stride=1, padding=2),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2),
            nn.Dropout(0.5)
        )
        self.layer3 = nn.Sequential(
            nn.Conv1d(in_channels=8, out_channels=4, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.AvgPool1d(kernel_size=2),
            nn.Dropout(0.5)
        )

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        return x

class CNNModel1D(nn.Module):
    def __init__(self, num_classes):
        super(CNNModel1D, self).__init__()
        self.base_cnn = CNNModel()
        self.dropout = nn.Dropout(0.5)
        self.fc = nn.Linear(4000, num_classes)

    def forward(self, x):
        x = self.base_cnn(x)
        x = x.view(x.size(0), -1)
        x = self.dropout(x)
        x = self.fc(x)
        return x

"""Architecture 2"""

import math
import torch
import torch.nn as nn
import torch.nn.functional as F

class BaseCNN(nn.Module):
    def __init__(self):
        super(BaseCNN, self).__init__()
        self.layer1 = nn.Sequential(
            nn.Conv1d(in_channels=9, out_channels=16, kernel_size=7, stride=1, padding=3),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=4),
        )
        self.layer2 = nn.Sequential(
            nn.Conv1d(in_channels=16, out_channels=8, kernel_size=5, stride=1, padding=2),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=4),
        )
        self.layer3 = nn.Sequential(
            nn.Conv1d(in_channels=8, out_channels=8, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.AvgPool1d(kernel_size=4),
        )

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        return x

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        self.encoding = self._generate_encoding(d_model, max_len)

    def _generate_encoding(self, d_model, max_len):
        encoding = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))
        encoding[:, 0::2] = torch.sin(position * div_term)
        encoding[:, 1::2] = torch.cos(position * div_term)
        return encoding

    def forward(self, x):

            seq_len = x.size(1)
            encoding = self.encoding[:seq_len, :].unsqueeze(0).to(x.device)
            return x + encoding




def scaled_dot_product(q, k, v, mask=None):
    d_k = q.size()[-1]
    attn_logits = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)
    if mask is not None:
        attn_logits = attn_logits.masked_fill(mask == 0, float('-inf'))
    attention = F.softmax(attn_logits, dim=-1)
    values = torch.matmul(attention, v)
    return values, attention

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.num_heads = num_heads
        self.d_model = d_model
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"

        self.depth = d_model // num_heads
        self.wq = nn.Linear(d_model, d_model)
        self.wk = nn.Linear(d_model, d_model)
        self.wv = nn.Linear(d_model, d_model)
        self.dense = nn.Linear(d_model, d_model)

    def split_heads(self, x, batch_size):
        x = x.view(batch_size, -1, self.num_heads, self.depth)
        return x.permute(0, 2, 1, 3)

    def forward(self, x, mask=None):
        batch_size = x.size(0)
        q = self.split_heads(self.wq(x), batch_size)
        k = self.split_heads(self.wk(x), batch_size)
        v = self.split_heads(self.wv(x), batch_size)

        scaled_attention, attention_weights = scaled_dot_product(q, k, v, mask)
        scaled_attention = scaled_attention.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)

        return self.dense(scaled_attention)

class EncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, dff, rate=0.1):
        super().__init__()
        self.mha = MultiHeadAttention(d_model, num_heads)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, dff),
            nn.ReLU(),
            nn.Linear(dff, d_model)
        )

        self.layernorm1 = nn.LayerNorm(d_model, eps=1e-6)
        self.layernorm2 = nn.LayerNorm(d_model, eps=1e-6)
        self.dropout1 = nn.Dropout(rate)
        self.dropout2 = nn.Dropout(rate)

    def forward(self, x, mask=None):
        attn_output = self.mha(x, mask)
        attn_output = self.dropout1(attn_output)
        out1 = self.layernorm1(x + attn_output)

        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output)
        out2 = self.layernorm2(out1 + ffn_output)

        return out2

class Encoder(nn.Module):
    def __init__(self, num_layers, d_model, num_heads, dff, rate=0.1):
        super().__init__()
        self.num_layers = num_layers
        self.enc_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)])
        self.dropout = nn.Dropout(rate)

    def forward(self, x, mask=None):
        for i in range(self.num_layers):
            x = self.enc_layers[i](x, mask)

        return x
class TransformerEncoder(nn.Module):
    def __init__(self, base_cnn, encoder, d_model, num_classes):
        super().__init__()
        self.base_cnn = base_cnn
        self.encoder = encoder
        self.pos_encoding = PositionalEncoding(d_model=d_model, max_len=5001)
        self.cnn_to_dmodel = nn.Linear(8, d_model)
        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))
        self.final_layer = nn.Linear(d_model, num_classes)

    def forward(self, x):
        batch_size = x.shape[0]
        x = self.base_cnn(x)
        x = x.permute(0, 2, 1)
        x = self.cnn_to_dmodel(x)

        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        x = torch.cat((cls_tokens, x), dim=1)
        x = self.pos_encoding(x)
        x = self.encoder(x)
        cls_token_output = x[:, 0, :]
        x = self.final_layer(cls_token_output)
        return x

"""Training"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install wandb --quite

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split

import wandb



def initialize_wandb(project_name):
    wandb.init(project=project_name)

def train(model, train_loader, optimizer, criterion, epochs=100, project_name="name"):
    initialize_wandb(project_name)
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        correct = 0
        total = 0
        for i, (inputs, labels) in enumerate(train_loader):
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        avg_loss = total / len(train_loader)
        accuracy = 100 * correct / total
        # Log metrics to WandB after each epoch
        wandb.log({"epoch": epoch+1, "loss": running_loss/len(train_loader), "accuracy": (correct/total)*100})
        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {(correct/total)*100:.2f}%')

    wandb.finish()

"""Evaluation"""

def evaluate(model, data_loader, criterion, mode='test'):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in data_loader:
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            running_loss += loss.item()

            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    accuracy = correct / total
    avg_loss = running_loss / len(data_loader)

    if mode == 'test':
        print(f'Test Accuracy: {accuracy*100:.2f}%, Test Loss: {avg_loss:.4f}')
    elif mode == 'validation':
        print(f'Validation Accuracy: {accuracy*100:.2f}%, Validation Loss: {avg_loss:.4f}')

    return accuracy, avg_loss

"""**K Fold Validation**"""

from torch.utils.data import DataLoader
import numpy as np

class KFoldCrossValidator:
    def __init__(self, model, criterion, optimizer, data_module, epochs=100, k=4):
        self.model = model
        self.criterion = criterion
        self.optimizer = optimizer
        self.data_module = data_module
        self.epochs = epochs
        self.k = k

    def train_fold(self, train_loader):
        self.model.train()
        for epoch in range(self.epochs):
            running_loss = 0.0
            correct = 0
            total = 0
            for inputs, labels in train_loader:
                self.optimizer.zero_grad()
                outputs = self.model(inputs)
                loss = self.criterion(outputs, labels)
                loss.backward()
                self.optimizer.step()
                running_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
            print(f'Epoch {epoch+1}/{self.epochs}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {(correct/total)*100:.2f}%')

    def validate_fold(self, val_loader):
        self.model.eval()
        running_loss = 0.0
        correct = 0
        total = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                outputs = self.model(inputs)
                loss = self.criterion(outputs, labels)
                running_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        accuracy = correct / total
        avg_loss = running_loss / len(val_loader)
        print(f'Validation Accuracy: {accuracy*100:.2f}%, Validation Loss: {avg_loss:.4f}')
        return accuracy, avg_loss

    def k_fold_cross_validation(self):
        fold_accuracies = []
        fold_losses = []
        for fold in range(self.k):
            print(f'Fold {fold+1}/{self.k}:')
            train_loader = self.data_module.train_dataloader()
            val_loader = self.data_module.val_dataloader()
            self.train_fold(train_loader)
            accuracy, loss = self.validate_fold(val_loader)
            fold_accuracies.append(accuracy)
            fold_losses.append(loss)
        avg_accuracy = np.mean(fold_accuracies)
        avg_loss = np.mean(fold_losses)
        print(f'Average Accuracy: {avg_accuracy*100:.2f}%, Average Loss: {avg_loss:.4f}')

"""Accuracy, Confusion matrix, F1-scores, and AUC-ROC curve"""

import torch
import numpy as np
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

class ModelEvaluator:
    def __init__(self, model, data_loader, num_classes):
        self.model = model
        self.data_loader = data_loader
        self.num_classes = num_classes
        self.predictions, self.true_labels, self.model_outputs = self.get_predictions()

    def get_predictions(self):
        self.model.eval()
        predictions = []
        true_labels = []
        outputs_list = []  # Store the outputs for AUC-ROC calculation
        with torch.no_grad():
            for inputs, labels in self.data_loader:
                outputs = self.model(inputs)
                softmaxes = torch.nn.functional.softmax(outputs, dim=1)
                _, predicted = torch.max(outputs, 1)
                predictions.extend(predicted.cpu().numpy())
                true_labels.extend(labels.cpu().numpy())
                outputs_list.extend(softmaxes.cpu().numpy())
        return np.array(predictions), np.array(true_labels), np.array(outputs_list)

    def calculate_accuracy(self):
        return accuracy_score(self.true_labels, self.predictions)

    def calculate_confusion_matrix(self):
        return confusion_matrix(self.true_labels, self.predictions)

    def calculate_f1_score(self, average='weighted'):
        return f1_score(self.true_labels, self.predictions, average=average)

    def calculate_auc_roc(self):
        true_one_hot = np.eye(self.num_classes)[self.true_labels]
        roc_auc = roc_auc_score(true_one_hot, self.model_outputs, multi_class="ovr", average="macro")
        return roc_auc

    def plot_roc_curve(self):
        true_one_hot = np.eye(self.num_classes)[self.true_labels]

        fpr = dict()
        tpr = dict()
        roc_auc = dict()
        for i in range(self.num_classes):
            fpr[i], tpr[i], _ = roc_curve(true_one_hot[:, i], self.model_outputs[:, i])
            roc_auc[i] = auc(fpr[i], tpr[i])

        # Plot all ROC curves
        plt.figure(figsize=(8, 6))
        colors = iter(plt.cm.rainbow(np.linspace(0, 1, self.num_classes)))
        for i, color in zip(range(self.num_classes), colors):
            plt.plot(fpr[i], tpr[i], color=color, lw=2,
                     label='Class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))

        plt.plot([0, 1], [0, 1], 'k--', lw=2)
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('Receiver Operating Characteristic (ROC) - Multi-class')
        plt.legend(loc="lower right")
        plt.show()

    def plot_confusion_matrix(self):
        cm = self.calculate_confusion_matrix()
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False, square=True)
        plt.xlabel('Predicted label')
        plt.ylabel('True label')
        plt.title('Confusion Matrix')
        plt.show()

    def evaluate(self):
        print(f'Accuracy: {self.calculate_accuracy() * 100:.2f}%')
        print('Confusion Matrix:\n', self.calculate_confusion_matrix())
        print(f'F1-Score: {self.calculate_f1_score():.2f}')
        print(f'AUC-ROC: {self.calculate_auc_roc():.2f}')
        self.plot_confusion_matrix()
        self.plot_roc_curve()

"""Trainable and Non-Trainable Parameters"""

import torch

def model_parameters(model):
    total_params = 0
    trainable_params = 0
    non_trainable_params = 0

    for param in model.parameters():
        total_params += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
        else:
            non_trainable_params += param.numel()

    print(f"Total Parameters: {total_params}")
    print(f"Trainable Parameters: {trainable_params}")
    print(f"Non-trainable Parameters: {non_trainable_params}")

model_architecture_1 = CNNModel1D(num_classes=10)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model_architecture_1.parameters(), lr=0.001)

# Train Architecture 1
train_loader = custom_data_module.train_dataloader()
train(model_architecture_1, train_loader, optimizer, criterion, project_name="dl_architecture1")

#Validation Architechture 1
val_loader = custom_data_module.val_dataloader()
val_accuracy, val_loss = evaluate(model_architecture_1, val_loader, criterion, mode='validation')

# Test Architecture 1
test_loader = custom_data_module.test_dataloader()
test_accuracy, test_loss = evaluate(model_architecture_1, test_loader, criterion, mode='test')

evaluator = ModelEvaluator(model_architecture_1, test_loader, num_classes=10)
evaluator.evaluate()

model_parameters(model_architecture_1)

"""K Fold Validation for CNN"""

validator = KFoldCrossValidator(
    model=model_architecture_1,
    criterion=criterion,
    optimizer=optimizer,
    data_module=custom_data_module,
    epochs=100,
    k=4
)
validator.k_fold_cross_validation()

"""Transformer Heads=4"""

base_cnn = BaseCNN()
encoder = Encoder(num_layers=2, d_model=128, num_heads=4, dff=2048, rate=0.1)

model_architecture_2_4 = TransformerEncoder(base_cnn=base_cnn, encoder=encoder, d_model=128, num_classes=10)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model_architecture_2_4.parameters(), lr=0.001)

train_loader = custom_data_module.train_dataloader()
train(model_architecture_2_4, train_loader, optimizer, criterion, epochs=100, project_name="dl_architecture2")

# Test Architecture 2
test_loader = custom_data_module.test_dataloader()
test_accuracy, test_loss = evaluate(model_architecture_2_4, test_loader, criterion, mode='test')

val_loader = custom_data_module.val_dataloader()
val_accuracy, val_loss = evaluate(model_architecture_2_4, val_loader, criterion, mode='validation')

evaluator = ModelEvaluator(model_architecture_2_4, test_loader, num_classes=10)
evaluator.evaluate()

model_parameters(model_architecture_2_4)

"""Transformer Head = 2"""

base_cnn = BaseCNN()
encoder = Encoder(num_layers=2, d_model=128, num_heads=2, dff=2048, rate=0.1)

model_architecture_2_2 = TransformerEncoder(base_cnn=base_cnn, encoder=encoder, d_model=128, num_classes=10)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model_architecture_2_2.parameters(), lr=0.001)

train_loader = custom_data_module.train_dataloader()
train(model_architecture_2_2, train_loader, optimizer, criterion, epochs=100, project_name="dl_architecture2")

# Test Architecture 2
test_accuracy, test_loss = evaluate(model_architecture_2_2, test_loader, criterion, mode='test')
val_accuracy, val_loss = evaluate(model_architecture_2_2, val_loader, criterion, mode='validation')

evaluator = ModelEvaluator(model_architecture_2_2, test_loader, num_classes=10)
evaluator.evaluate()

model_parameters(model_architecture_2_2)

"""Transformer Head=1"""

base_cnn = BaseCNN()
encoder = Encoder(num_layers=2, d_model=128, num_heads=1, dff=2048, rate=0.1)

model_architecture_2_1 = TransformerEncoder(base_cnn=base_cnn, encoder=encoder, d_model=128, num_classes=10)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model_architecture_2_1.parameters(), lr=0.001)

train_loader = custom_data_module.train_dataloader()
train(model_architecture_2_1, train_loader, optimizer, criterion, epochs=100, project_name="dl_architecture2")

test_accuracy, test_loss = evaluate(model_architecture_2_1, test_loader, criterion, mode='test')
val_accuracy, val_loss = evaluate(model_architecture_2_1, val_loader, criterion, mode='validation')

evaluator = ModelEvaluator(model_architecture_2_1, test_loader, num_classes=10)
evaluator.evaluate()

model_parameters(model_architecture_2_1)

"""K-Fold Validation for Transformer"""

# K Flod Validation For 1 head
validator_transformer1 = KFoldCrossValidator(
    model=model_architecture_2_1,
    criterion=criterion,
    optimizer=optimizer,
    data_module=custom_data_module,
    epochs=100,
    k=4
)
validator_transformer1.k_fold_cross_validation()

# K Flod Validation For 2 head
validator_transformer2 = KFoldCrossValidator(
    model=model_architecture_2_2,
    criterion=criterion,
    optimizer=optimizer,
    data_module=custom_data_module,
    epochs=100,
    k=4
)
validator_transformer2.k_fold_cross_validation()

# K Flod Validation For 4 head
validator_transformer4 = KFoldCrossValidator(
    model=model_architecture_2_4,
    criterion=criterion,
    optimizer=optimizer,
    data_module=custom_data_module,
    epochs=100,
    k=4
)
validator_transformer4.k_fold_cross_validation()

"""Hyperparameter tuning

CNN
"""

import torch.optim as optim

optimizers = {
    'Adam': optim.Adam,
    'RMSprop': optim.RMSprop
}

batch_sizes = [32, 64]


valid_samp = 2  # Validation sample
test_samp = 1   # Test sample
num_workers = 2 # Adjust as necessary

performance_metrics = {}

for opt_name, opt_class in optimizers.items():
    for batch_size in batch_sizes:
        print(f"\nTraining with {opt_name} optimizer and batch size {batch_size}")

        my_custom_data_module = CustomDataModule(batch_size=batch_size,
                                      num_workers=num_workers,
                                      data_directory=path,
                                      data_frame=df,
                                      validation_fold=valid_samp,
                                      testing_fold=test_samp,
                                      esc_10_flag=True,
                                      file_column='filename',
                                      label_column='category',
                                      sampling_rate=44100,
                                      new_sampling_rate=16000,
                                      sample_length_seconds=1)

        my_custom_data_module.setup()

        model_architecture_1_hyperparameter = CNNModel1D(num_classes=10)
        criterion = nn.CrossEntropyLoss()

        optimizer = opt_class(model_architecture_1_hyperparameter.parameters(), lr=0.001)

        train_loader = my_custom_data_module.train_dataloader()
        train(model_architecture_1_hyperparameter, train_loader, optimizer, criterion, project_name=f"{opt_name}_{batch_size}")

        val_loader = my_custom_data_module.val_dataloader()
        test_loader = my_custom_data_module.test_dataloader()
        val_accuracy, val_loss = evaluate(model_architecture_1_hyperparameter, val_loader, criterion, mode='validation')
        test_accuracy, test_loss = evaluate(model_architecture_1_hyperparameter, test_loader, criterion, mode='test')

        performance_metrics[(opt_name, batch_size, 'val')] = (val_accuracy, val_loss)
        performance_metrics[(opt_name, batch_size, 'test')] = (test_accuracy, test_loss)

best_val_combination = max(performance_metrics, key=lambda x: performance_metrics[x][0] if x[2] == 'val' else 0)
best_val_performance = performance_metrics[best_val_combination]

print(f"\nBest Validation Combination: Optimizer={best_val_combination[0]}, Batch Size={best_val_combination[1]}")
print(f"Validation Accuracy: {best_val_performance[0]*100:.2f}%, Validation Loss: {best_val_performance[1]:.4f}")

best_test_performance = performance_metrics[(best_val_combination[0], best_val_combination[1], 'test')]
print(f"Test Accuracy: {best_test_performance[0]*100:.2f}%, Test Loss: {best_test_performance[1]:.4f}")

"""Transformer"""

# 1 head

import torch.optim as optim

optimizers = {
    'Adam': optim.Adam,
    'RMSprop': optim.RMSprop
}

batch_sizes = [32, 64]

valid_samp = 2  # Validation sample
test_samp = 1   # Test sample
num_workers = 2 # Adjust as necessary

performance_metrics = {}

for opt_name, opt_class in optimizers.items():
    for batch_size in batch_sizes:
        print(f"\nTraining with {opt_name} optimizer and batch size {batch_size}")

        my_custom_data_module = CustomDataModule(batch_size=batch_size,
                                      num_workers=num_workers,
                                      data_directory=path,
                                      data_frame=df,
                                      validation_fold=valid_samp,
                                      testing_fold=test_samp,  # set to 0 for no test set
                                      esc_10_flag=True,
                                      file_column='filename',
                                      label_column='category',
                                      sampling_rate=44100,
                                      new_sampling_rate=16000,  # new sample rate for input
                                      sample_length_seconds=1  # new length of input in seconds
                                      )
        my_custom_data_module.setup()

        base_cnn = BaseCNN()
        encoder = Encoder(num_layers=2, d_model=128, num_heads=1, dff=2048, rate=0.1)
        model_architecture_2_1_hyperparameter = TransformerEncoder(base_cnn=base_cnn, encoder=encoder, d_model=128, num_classes=10)

        criterion = nn.CrossEntropyLoss()

        optimizer = opt_class(model_architecture_2_1_hyperparameter.parameters(), lr=0.001)

        train_loader = my_custom_data_module.train_dataloader()
        train(model_architecture_2_1_hyperparameter, train_loader, optimizer, criterion, epochs=100, project_name=f"{opt_name}_{batch_size}")

        val_loader = my_custom_data_module.val_dataloader()
        test_loader = my_custom_data_module.test_dataloader()
        val_accuracy, val_loss = evaluate(model_architecture_2_1_hyperparameter, val_loader, criterion, mode='validation')
        test_accuracy, test_loss = evaluate(model_architecture_2_1_hyperparameter, test_loader, criterion, mode='test')

        performance_metrics[(opt_name, batch_size, 'val')] = (val_accuracy, val_loss)
        performance_metrics[(opt_name, batch_size, 'test')] = (test_accuracy, test_loss)

best_val_combination = max(performance_metrics, key=lambda x: performance_metrics[x][0] if x[2] == 'val' else 0)
best_val_performance = performance_metrics[best_val_combination]

print(f"\nBest Validation Combination: Optimizer={best_val_combination[0]}, Batch Size={best_val_combination[1]}")
print(f"Validation Accuracy: {best_val_performance[0]*100:.2f}%, Validation Loss: {best_val_performance[1]:.4f}")

best_test_performance = performance_metrics[(best_val_combination[0], best_val_combination[1], 'test')]
print(f"Test Accuracy: {best_test_performance[0]*100:.2f}%, Test Loss: {best_test_performance[1]:.4f}")

"""Transformer 2 Head"""

import torch.optim as optim

optimizers = {
    'Adam': optim.Adam,
    'RMSprop': optim.RMSprop
}

batch_sizes = [32, 64]

valid_samp = 2  # Validation sample
test_samp = 1   # Test sample
num_workers = 2 # Adjust as necessary

performance_metrics = {}

for opt_name, opt_class in optimizers.items():
    for batch_size in batch_sizes:
        print(f"\nTraining with {opt_name} optimizer and batch size {batch_size}")

        my_custom_data_module = CustomDataModule(batch_size=batch_size,
                                      num_workers=num_workers,
                                      data_directory=path,
                                      data_frame=df,
                                      validation_fold=valid_samp,
                                      testing_fold=test_samp,  # set to 0 for no test set
                                      esc_10_flag=True,
                                      file_column='filename',
                                      label_column='category',
                                      sampling_rate=44100,
                                      new_sampling_rate=16000,  # new sample rate for input
                                      sample_length_seconds=1  # new length of input in seconds
                                      )
        my_custom_data_module.setup()

        base_cnn = BaseCNN()
        encoder = Encoder(num_layers=2, d_model=128, num_heads=2, dff=2048, rate=0.1)
        model_architecture_2_2_hyperparameter = TransformerEncoder(base_cnn=base_cnn, encoder=encoder, d_model=128, num_classes=10)

        criterion = nn.CrossEntropyLoss()

        optimizer = opt_class(model_architecture_2_2_hyperparameter.parameters(), lr=0.001)

        # Train the model
        train_loader = my_custom_data_module.train_dataloader()
        train(model_architecture_2_2_hyperparameter, train_loader, optimizer, criterion, epochs=100, project_name=f"{opt_name}_{batch_size}")

        val_loader = my_custom_data_module.val_dataloader()
        test_loader = my_custom_data_module.test_dataloader()
        val_accuracy, val_loss = evaluate(model_architecture_2_2_hyperparameter, val_loader, criterion, mode='validation')
        test_accuracy, test_loss = evaluate(model_architecture_2_2_hyperparameter, test_loader, criterion, mode='test')

        performance_metrics[(opt_name, batch_size, 'val')] = (val_accuracy, val_loss)
        performance_metrics[(opt_name, batch_size, 'test')] = (test_accuracy, test_loss)

best_val_combination = max(performance_metrics, key=lambda x: performance_metrics[x][0] if x[2] == 'val' else 0)
best_val_performance = performance_metrics[best_val_combination]

print(f"\nBest Validation Combination: Optimizer={best_val_combination[0]}, Batch Size={best_val_combination[1]}")
print(f"Validation Accuracy: {best_val_performance[0]*100:.2f}%, Validation Loss: {best_val_performance[1]:.4f}")

best_test_performance = performance_metrics[(best_val_combination[0], best_val_combination[1], 'test')]
print(f"Test Accuracy: {best_test_performance[0]*100:.2f}%, Test Loss: {best_test_performance[1]:.4f}")

"""Transformer 4 Head"""

import torch.optim as optim

# Define optimizers
optimizers = {
    'Adam': optim.Adam,
    'RMSprop': optim.RMSprop
}

batch_sizes = [32, 64]

valid_samp = 2  # Validation sample
test_samp = 1   # Test sample
num_workers = 2 # Adjust as necessary

performance_metrics = {}

for opt_name, opt_class in optimizers.items():
    for batch_size in batch_sizes:
        print(f"\nTraining with {opt_name} optimizer and batch size {batch_size}")

        my_custom_data_module = CustomDataModule(batch_size=batch_size,
                                      num_workers=num_workers,
                                      data_directory=path,
                                      data_frame=df,
                                      validation_fold=valid_samp,
                                      testing_fold=test_samp,  # set to 0 for no test set
                                      esc_10_flag=True,
                                      file_column='filename',
                                      label_column='category',
                                      sampling_rate=44100,
                                      new_sampling_rate=16000,  # new sample rate for input
                                      sample_length_seconds=1  # new length of input in seconds
                                      )
        my_custom_data_module.setup()

        base_cnn = BaseCNN()
        encoder = Encoder(num_layers=2, d_model=128, num_heads=4, dff=2048, rate=0.1)
        model_architecture_2_4_hyperparameter = TransformerEncoder(base_cnn=base_cnn, encoder=encoder, d_model=128, num_classes=10)

        criterion = nn.CrossEntropyLoss()

        optimizer = opt_class(model_architecture_2_4_hyperparameter.parameters(), lr=0.001)

        train_loader = my_custom_data_module.train_dataloader()
        train(model_architecture_2_4_hyperparameter, train_loader, optimizer, criterion, epochs=100, project_name=f"{opt_name}_{batch_size}")

        val_loader = my_custom_data_module.val_dataloader()
        test_loader = my_custom_data_module.test_dataloader()
        val_accuracy, val_loss = evaluate(model_architecture_2_4_hyperparameter, val_loader, criterion, mode='validation')
        test_accuracy, test_loss = evaluate(model_architecture_2_4_hyperparameter, test_loader, criterion, mode='test')

        performance_metrics[(opt_name, batch_size, 'val')] = (val_accuracy, val_loss)
        performance_metrics[(opt_name, batch_size, 'test')] = (test_accuracy, test_loss)

best_val_combination = max(performance_metrics, key=lambda x: performance_metrics[x][0] if x[2] == 'val' else 0)
best_val_performance = performance_metrics[best_val_combination]

print(f"\nBest Validation Combination: Optimizer={best_val_combination[0]}, Batch Size={best_val_combination[1]}")
print(f"Validation Accuracy: {best_val_performance[0]*100:.2f}%, Validation Loss: {best_val_performance[1]:.4f}")

best_test_performance = performance_metrics[(best_val_combination[0], best_val_combination[1], 'test')]
print(f"Test Accuracy: {best_test_performance[0]*100:.2f}%, Test Loss: {best_test_performance[1]:.4f}")